{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93834311681711\n",
      "0.7211047102874315\n",
      "0.9778024140774093\n",
      "0.7808688094430302\n",
      "0.861365903383803\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_points: dict[str, np.ndarray] = {\n",
    "    \"USA\": np.array([5, 6]),\n",
    "    \"Washington\": np.array([10, 5]),\n",
    "    \"Turkey\": np.array([3, 1]),\n",
    "    \"Ankara\": np.array([9, 1]),\n",
    "    \"Russia\": np.array([5, 5]),\n",
    "    \"Japan\": np.array([4, 3]),\n",
    "}\n",
    "\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "# Find the cosine similarity between the vectors of the following pairs of data points with \"Ankara\" and \"Washington\"\n",
    "print(cosine_similarity(data_points[\"Ankara\"], data_points[\"Washington\"]))\n",
    "print(cosine_similarity(data_points[\"Ankara\"], data_points[\"USA\"]))\n",
    "print(cosine_similarity(data_points[\"Ankara\"], data_points[\"Turkey\"]))\n",
    "print(cosine_similarity(data_points[\"Ankara\"], data_points[\"Russia\"]))\n",
    "print(cosine_similarity(data_points[\"Ankara\"], data_points[\"Japan\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4],\n",
       "       [4, 4]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[2, 2], [2, 2]])\n",
    "A_squared = np.square(A)\n",
    "A_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_Frobenius = np.sqrt(np.sum(A_squared))\n",
    "A_Frobenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.array([2, 2])\n",
    "v = np.array([0, 1])\n",
    "dot_product = np.dot(P, v.T)\n",
    "print(dot_product)\n",
    "sign_of_dot_product = np.sign(dot_product)\n",
    "sign_of_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding = {\n",
    "    \"I\": np.array([1, 0, 0]),\n",
    "    \"love\": np.array([0, 1, 0]),\n",
    "    \"deep\": np.array([0, 0, 1]),\n",
    "    \"learning\": np.array([1, 1, 1]),\n",
    "}\n",
    "\n",
    "words_in_document = [\"I\", \"love\", \"deep\", \"learning\"]\n",
    "document_embedding = np.array([0, 0, 0])\n",
    "\n",
    "for word in words_in_document:\n",
    "    document_embedding += word_embedding[word]\n",
    "\n",
    "document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3.]\n",
      " [1. 2. 3. 4.]\n",
      " [2. 3. 4. 5.]\n",
      " [3. 4. 5. 4.]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Minimum edit distance algorithm between two strings\n",
    "\n",
    "\n",
    "def min_edit_distance(\n",
    "    source: str,\n",
    "    target: str,\n",
    "    insertion_cost: int = 1,\n",
    "    deletion_cost: int = 1,\n",
    "    substitution_cost: int = 2,\n",
    ") -> int:\n",
    "    # Create a matrix and initialize with zeros\n",
    "    matrix = np.zeros((len(source) + 1, len(target) + 1))\n",
    "\n",
    "    # Initialize the first row and column\n",
    "    for i in range(1, len(source) + 1):\n",
    "        matrix[i][0] = matrix[i - 1][0] + deletion_cost\n",
    "    for j in range(1, len(target) + 1):\n",
    "        matrix[0][j] = matrix[0][j - 1] + insertion_cost\n",
    "\n",
    "    # Fill in the matrix\n",
    "    for i in range(1, len(source) + 1):\n",
    "        for j in range(1, len(target) + 1):\n",
    "            if source[i - 1] == target[j - 1]:\n",
    "                substitution = 0\n",
    "            else:\n",
    "                substitution = substitution_cost\n",
    "            matrix[i][j] = min(\n",
    "                matrix[i - 1][j] + deletion_cost,\n",
    "                matrix[i][j - 1] + insertion_cost,\n",
    "                matrix[i - 1][j - 1] + substitution,\n",
    "            )\n",
    "    print(matrix)\n",
    "    return int(matrix[len(source)][len(target)])\n",
    "\n",
    "\n",
    "# Test the function with some examples\n",
    "print(min_edit_distance(\"Pie\", \"Bye\"))  # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi algorithm for finding the most likely sequence of hidden states in a Hidden Markov Model (HMM)\n",
    "\n",
    "\n",
    "def viterbi(\n",
    "    observations: np.ndarray,\n",
    "    states: np.ndarray,\n",
    "    start_probability: np.ndarray,\n",
    "    transition_probability: np.ndarray,\n",
    "    emission_probability: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    num_observations = len(observations)\n",
    "    num_states = len(states)\n",
    "    viterbi_matrix = np.zeros((num_states, num_observations))\n",
    "    backpointer = np.zeros((num_states, num_observations))\n",
    "\n",
    "    # Initialize the first column of the viterbi matrix\n",
    "    for i in range(num_states):\n",
    "        viterbi_matrix[i, 0] = (\n",
    "            start_probability[i] * emission_probability[i, observations[0]]\n",
    "        )\n",
    "        backpointer[i, 0] = 0\n",
    "\n",
    "    # Fill in the viterbi matrix\n",
    "    for t in range(1, num_observations):\n",
    "        for s in range(num_states):\n",
    "            viterbi_values = np.zeros(num_states)\n",
    "            for s_prime in range(num_states):\n",
    "                viterbi_values[s_prime] = (\n",
    "                    viterbi_matrix[s_prime, t - 1]\n",
    "                    * transition_probability[s_prime, s]\n",
    "                    * emission_probability[s, observations[t]]\n",
    "                )\n",
    "            viterbi_matrix[s, t] = np.max(viterbi_values)\n",
    "            backpointer[s, t] = np.argmax(viterbi_values)\n",
    "\n",
    "    # Find the most likely final state\n",
    "    final_state = np.argmax(viterbi_matrix[:, num_observations - 1])\n",
    "    hidden_states = np.zeros(num_observations)\n",
    "    hidden_states[num_observations - 1] = final_state\n",
    "\n",
    "    # Backtrack to find the most likely sequence of hidden states\n",
    "    for t in range(num_observations - 2, -1, -1):\n",
    "        hidden_states[t] = backpointer[int(hidden_states[t + 1]), t + 1]\n",
    "\n",
    "    return hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
